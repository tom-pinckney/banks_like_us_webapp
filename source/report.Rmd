---
title: "Banks Like This One"
author: "Tom Pinckney"
date: "May 11, 2019"
output:
  rmarkdown::html_document:
    theme: cerulean
    toc: true
    toc_float: true
    code_folding: show
---
```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Starting Point
The goal of this project is to create a model that returns a list of similar banks for any given bank using any available public data

**Where Would you Start?**
I started by researching other bank grouping/clustering methods and reading up on the data at the FDIC website. My research helped me decide to focus on bank call report summaries as my starting point.

**What information do you wish you had?**
Throughout the entire project I wished I had someone with domain expertise to talk to. No amount of statistical knowledge or data science skills can make up for a lack of domain understanding. For many data sets I also wished I had at least a simple data dictionary to explain many of the variables.

**In the absence of perfect knowledge and access to perfect data, what can you do to make a first attempt?**
I chose to use a subset of numeric data that I felt I understood to create a distance matrix of all banks. I then used some FDIC categorical groupings (bank specialization and bank class) as additional parameters to filter the distance matrix.

This allowed me to create a simple but seemingly effective function that takes any bank FDIC certificate number and returns
up to 100 of the most similar banks with an option to limit output to banks in the same charter class and/or specialization. 


## Model Building
There is a massive amount of data at the FDIC [bulk data download](https://cdr.ffiec.gov/public/PWS/DownloadBulkData.aspx). Without subject matter knowledge it is very difficult to decide what is important and what is not. Based on outside research we'll start with the call data subset. This download contains aggregated call report data on all active banks. Most of the data is somewhat interpretable by reading the column names which is important because there is no data dictionary. 

This data provides a good starting place but is lacking some variables that could be helpful such as number of branches and age of the banks. There is another data set on the FDIC website [here](https://banks.data.fdic.gov/docs/) containing institution level data that can fill in many of the gaps.

### Data Loading and Exploring
First we'll read in the data and check the dimensions. 
```{r read in data, message=FALSE, warning=FALSE}
library(tidyverse)
library(here)

# set seed for reproducibility
set.seed(2119)

# Read in institution data and filter to active banks
institutions <- read_csv(here::here('data', 'raw', 'institutions_locations', 'institutions.csv')) %>% 
  rename_all(str_to_lower) %>% 
  filter(active == 1)

# Read in call report data sets
files <- list.files(here::here('data', 'raw', 'call_data_subset'))
files <- files[!str_detect(files, "Readme")]

data_list <- map(files, ~read_tsv(here::here('data', 'raw', 'call_data_subset', .x)))

# The first row of the call data is really more column names we need to remove these and add them to the current names
# We'll also remove NA from the column names and replace whitespace with _ 
add_names <- function(df) {
  names(df) <- str_replace_all(paste(colnames(df), as.character(unlist(df[1,]))), c(" NA" = "", " " = "_"))
  df[-1,]
}

# fix the names by applying the function
data_list <- map(data_list, add_names)

# Visual review shows these datasets have info about the same banks, join into one data set
full_call_data <- data_list[[1]] %>% 
  inner_join(data_list[[2]]) %>% 
  rename_all(str_to_lower)

# check
print(paste("Data Dimensions",c("Institutions","Call Reports"), map(list(institutions, full_call_data), dim)))
```
There are around 4X rows in the call reports data because there are multiple rows per bank, one row for each quarter of 2018. We'll only use the most recent data from Q4 in order to have the most recent snapshot of every bank.
```{r q4 2018, message=FALSE, warning=FALSE}
library(lubridate)

# Filter to Q4
call_data_q4 <- full_call_data %>% 
  filter(reporting_period_end_date == date('2018-12-31'))

# check
print(paste("Data Dimensions",c("Institutions","Call Reports"), map(list(institutions, call_data_q4), dim)))
```
There are still some differences in numbers of rows/banks in each data set. There are 13 banks in the institutions data set not found in the call data mostly international banks. There are 103 banks in the call report data not in the institutions data set. For now we will just drop these banks.


### Examing Variation
For our MVP model we will take two approaches:

* Create a euclidean distance matrix from understandable numeric non-missing data relating to bank income, expenses, and size
* Use categorical bank groupings as optional filters on model output

We will avoid missing values because they are likely not missing at random and we do not have time to diagnose the underlying reason for the missing data. It may be that different banks are required to report different metrics to the FDIC making the missingness meaningful. To try and keep some of the information the missingness contains we'll add up all the number of missing values for each bank and use it as a variable in our model.
```{r selecting columns}
# Create number missing column and select some variables relating to overall bank size, income, and expenses
call_data_selections <- call_data_q4 %>% 
  mutate(n_missing = rowSums(is.na(call_data_q4))) %>% 
  select(cert = fdic_certificate_number, rcon2200_total_deposits, riad4073_total_interest_expense, riad4079_total_noninterest_income, riad4093_total_noninterest_expense, riad4107_total_interest_income, riad4340_net_income, riad4135_salaries_and_employee_benefits, n_missing, riad4092_other_noninterest_expense, riad4518_other_interest_income) %>% 
  mutate_all(as.numeric)

# Create age variable and select other variables relating to overall bank size, income, and expenses
# Also select categorical variables that will be used as optional filters on model output
institution_selections <- institutions %>% 
  mutate(age = as.numeric(today() - date(estymd)) / 365) %>% 
  select(name, cert, asset, age, roa, roe, offices, specialization = specgrpn, bank_class = bkclass)

# Join data sets together
mod_data <- call_data_selections %>% 
  inner_join(institution_selections, by = "cert")
```

Before modeling we'll explore our chosen variables starting with our categorical features. We picked bank charter class and specialization as our categorical variables. These provide broad categories of banks that may be useful in determining what banks are similar to each other. Bank charter class has five levels:

* N = commercial bank, national (federal) charter and Fed member, supervised by the Office of the Comptroller of the Currency (OCC) 
* SM = commercial bank, state charter and Fed member, supervised by the Federal Reserve (FRB)  
* NM = commercial bank, state charter and Fed nonmember, supervised by the FDIC  
* SB = savings banks, state charter, supervised by the FDIC  
* SA = savings associations, state or federal charter, supervised by the Office of Thrift Supervision (OTS) 
* OI = insured U.S. branch of a foreign chartered institution (IBA)

While there are nine self explanatory levels in specialization.
```{r categorical}
mod_data %>% 
    count(specialization) %>% 
  ggplot(aes(reorder(specialization, n), n)) +
  geom_col() +
  geom_text(aes(label =  n), hjust = 0) +
  coord_flip() +
  expand_limits(y = 3050) +
  theme_minimal() +
  labs(y = "",
       x = "",
       title = "Most banks have Commercial or Agricultural Specialization")

mod_data %>% 
    count(bank_class) %>% 
  ggplot(aes(reorder(bank_class, n), n)) +
  geom_col() +
  geom_text(aes(label =  n), hjust = 0) +
  coord_flip() +
  expand_limits(y = 3050) +
  theme_minimal() +
  labs(y = "",
       x = "",
       title = "Most Banks have a State Charter") +
  expand_limits(y = 3150)
```

Combing these variables gives us 40 groupings that can be used to subset the output of our distance matrix model. 
```{r, fig.width=12, fig.height=8}
mod_data %>% 
  count(bank_class, specialization) %>% 
  mutate(class_specialization = paste(bank_class, specialization)) %>% 
  ggplot(aes(reorder(class_specialization, n), n)) +
  geom_col() +
  geom_text(aes(label = n), hjust = -0.1) +
  coord_flip() +
  theme_minimal() +
  labs(y = "",
       x = "",
       title = "Categorical combinations can help split banks into similar groups")
```
There are likely other categorical variables that could be useful in further splitting banks, but for now we will move forward with just these two. 

Now lets move on to examining the spread in our numeric data
```{r}
mod_data %>% 
  select_if(is.numeric) %>% 
  select(-cert) %>% 
  gather(column, value, 1:ncol(.)) %>% 
  mutate(short_name = map_chr(str_split(column, "_"), ~.x[[1]])) %>% 
  ggplot(aes(short_name, value)) +
  geom_boxplot() +
  coord_flip() +
  theme_minimal() +
  labs(y = "",
       x = "",
       title = "Variables appear highly skewed",
       subtitle = "Big banks are MUCH bigger")
```

We see a lot of right skewing in our numeric variables. This makes sense, since many of these metrics change as banks get bigger. We could fix this with a log transformation, however, we would have to take care of negative values first. One way to do this is by adding min(x) + 1. When we do this we get more normal distributions. 
```{r}
# Box plot with log transofmration
mod_data %>% 
  select_if(is.numeric) %>% 
  select(-cert) %>% 
  mutate_all(function(x) log((abs(min(x)) + 1) + x)) %>% 
  gather(column, value, 1:ncol(.)) %>% 
  mutate(short_name = map_chr(str_split(column, "_"), ~.x[[1]])) %>% 
  ggplot(aes(short_name, value)) +
  geom_boxplot() +
  coord_flip() +
  theme_minimal() +
  labs(y = "",
       x = "",
       title = "Log transformation makes data more normal",
       subtitle = "But fundamentally changes data, no more negative values")
```
The problem is we've now fundamentally changed out data. Without subject matter expertise we should be hesitant to do this. For MVP 1 we'll move forward with the untransformed data. 

Next we'll examine how the columns we picked separate the data using PCA.
```{r plot PCA, message=FALSE, warning=FALSE}
library(cluster)
library(ggfortify)

# Standardize data
mod_data_std <- mod_data %>% 
  select(-cert, -name, -specialization, -bank_class) %>% 
  mutate_all(scale) %>% 
  mutate_all(as.numeric)

# Get labels for plotting
labels <- mod_data %>% 
  mutate(name = if_else(cert %in% c(213, 3510, 3511, 628, 33831), word(str_replace(name, "of ", ""), 1, 2), NA_character_)) %>% 
  select(name)


autoplot(prcomp(mod_data_std, center = FALSE, scale. = FALSE),
         loadings = TRUE,
         label = TRUE,
         label.repel = TRUE,
         label.label = labels$name,
         data = mod_data) + 
  theme_bw() +
  labs(title = "First two principal components of PCA on selected data",
       subtitle = "Most variables spread data in the same direction") +
  annotate(geom = "text", x = 0.07, y = -0.50, label = "Return on \nAssets/Equity", color = 'red') +
  annotate(geom = "text", x = 0.22, y = -0.07, label = "Total\nDeposits/Income/Interest", color = 'red') +
  annotate(geom = "text", x = -0.05, y = -0.03, label = "Missing\nVals", color = 'red') +
  annotate(geom = "text", x = 0.06, y = 0.1, label = "Bank Age", color = 'red')
```

Most of our variables (About total numbers) all pull the data in the same direction. This is not necessarily a bad thing for our model, co-linearity should not have negative effects. However, it does mean our data does not have as much spread in multiple dimensions as we would like. 

We could try and bring in some more variables from the call report data. 
```{r bring in other vars, message=FALSE, warning=FALSE}
other_vars <- call_data_q4 %>% 
  select(starts_with("riad"),starts_with("rcon"), starts_with('rcfd')) %>%
  mutate_all(as.numeric) %>% 
  select_if(~ !any(is.na(.)))

# Drop any columns already in our mod_data
other_vars <- other_vars[ , !colnames(other_vars) %in% colnames(mod_data)] %>% 
  mutate_all(scale)%>% 
  select_if(~ !any(is.na(.)))

labels <- call_data_q4 %>% 
  mutate(financial_institution_name = if_else(fdic_certificate_number %in% c(213, 3510, 3511, 628, 33831), word(str_replace(financial_institution_name, "of ", ""), 1, 2), NA_character_)) %>% select(financial_institution_name)

autoplot(prcomp(other_vars, center = FALSE, scale. = FALSE), 
         loadings = TRUE,
         label = TRUE,
         label.repel = TRUE,
         label.label = labels$financial_institution_name)+ 
  theme_bw() +
  labs(title = "Most variables pull data in the same direction",
       subtitle = "PC1 captures 45% of variation while PC2 only captures 6.1%") +
  annotate(geom = "text", x = 0, y = -0.45, label = "rconk115", color = 'red')+
  annotate(geom = "text", x = -0.06, y = -0.58, label = "riad3196", color = 'red')
```
Most of the variables from the call reports seem to all pull in the same direction, but there are two variables (riad3196, rconk115) that pull in different directions mostly along PC2. We'll add these two variables to our data and move on with our MVP. If time permits we can research more sources of data (such as bank geography) that may add new dimensions to the variation in our data set. 

```{r add variables to data}
vars_to_add <- call_data_q4 %>% 
  select(cert = fdic_certificate_number, `riad3196_relzd_gains(loss)_on_avl-fr-sle_secs`, rconk115_ln_rstr_secd_nonfarm_own_pd_90) %>% 
  mutate_all(as.numeric)

mod_data_new_vars <- mod_data %>% 
  inner_join(vars_to_add, by = "cert")


mod_data_std_new_vars <- mod_data_new_vars %>% 
  select(-cert, -name, -specialization, -bank_class) %>% 
  mutate_all(scale) %>% 
  mutate_all(as.numeric)

print(paste(c("New data dim", "Old Data dim"), map(list(mod_data_std_new_vars, mod_data_std), dim)))
```


### Model Building
We'll start by creating a distance matrix and convert it to a long data frame of bank similarities. We'll then pull in the categorical variables we want to use to filter our model output.
```{r distance matrix}
write_csv(mod_data_new_vars, here::here('data', 'output', 'model-data.csv'))

# Create the distance matrix
dm <- daisy(mod_data_std_new_vars, 
            metric = 'euclidean',
            stand = FALSE) %>% 
  as.matrix()

# Add ID back to distance matrix
rownames(dm) <- mod_data$cert
colnames(dm) <- mod_data$cert

# Convert to long data set for use in function
# Limit to 100 closest banks for ease of use
bank_similarities <- dm %>% 
  as_tibble(rownames = "cert_1") %>% 
  gather(cert_2, distance, 2:ncol(.)) %>% 
  filter(distance > 0) %>% 
  group_by(cert_1) %>% 
  mutate(ave_distance = mean(distance)) %>% 
  top_n(-100, distance)

# Select other data to add to our bank similarities data set 
bank_meta <- institutions %>% 
  mutate(cert = as.character(cert)) %>% 
  select(cert, name_n = name, bank_class_n = bkclass, specialization_n = specgrpn)

# Join it in to our similarities data set
bank_similarities <- bank_similarities %>% 
  inner_join(bank_meta %>% 
               rename(name_1 = name_n,
                      bank_class_1 = bank_class_n,
                      specialization_1 = specialization_n),
             by = c("cert_1" = "cert")) %>% 
  inner_join(bank_meta %>% 
               rename(name_2 = name_n,
                      bank_class_2 = bank_class_n,
                      specialization_2 = specialization_n),
             by = c("cert_2" = "cert")) %>% 
  select(cert_1, name_1, cert_2, name_2, distance, bank_class_1, bank_class_2, specialization_1, specialization_2, ave_distance) %>% 
  ungroup()

write_csv(bank_similarities, here::here('data', 'output', 'bank_similarites.csv'))
```
Finally we'll define a function to return similar banks to any given bank with an option to use bank class or specialization to limit 
returned banks. 
```{r function}
get_similar_banks <- function(FDIC_certification_number, n_banks = 20, limit_to_bank_class = FALSE, limit_to_specialization = FALSE){
  # Function returns list of similar banks as defined by our distance matrix for any FDIC certificate number
  # n_banks number of similar banks to return
  # limit_to_bank_class if TRUE limits results to banks in the same bank class
  # limit_to_specialization if TRUE limits results to banks in the same specialization
  
  if(n_banks > 100){
    stop("Cannot return more than 100 similar banks")
  }
  
  if(!FDIC_certification_number %in% unique(bank_similarities$cert_1)){
    stop("No bank with given FDIC Certification number found")
  }
  
  FDIC_certification_number <- as.character(FDIC_certification_number)
  
  if(limit_to_bank_class == TRUE & limit_to_specialization == TRUE){
    
    similar_banks <- bank_similarities %>% 
      filter(cert_1 == FDIC_certification_number) %>% 
      filter(bank_class_1 == bank_class_2) %>% 
      filter(specialization_1 == specialization_2) %>% 
      arrange(distance) %>% 
      slice(1:n_banks) %>% 
      select(Bank = name_1, `Similar Banks` = name_2, distance, Specialization = specialization_1, `Bank Class` = bank_class_1)
    
  }
  
  if(limit_to_bank_class == TRUE & limit_to_specialization == FALSE){
    
    similar_banks <- bank_similarities %>% 
      filter(cert_1 == FDIC_certification_number) %>% 
      filter(bank_class_1 == bank_class_2) %>% 
      arrange(distance) %>% 
      slice(1:n_banks) %>% 
      select(Bank = name_1, `Similar Banks` = name_2, distance, `Bank Class` = bank_class_1)
    
  }
  
  if(limit_to_bank_class == FALSE & limit_to_specialization == TRUE){
    
    similar_banks <- bank_similarities %>% 
      filter(cert_1 == FDIC_certification_number) %>% 
      filter(specialization_1 == specialization_2) %>% 
      arrange(distance) %>% 
      slice(1:n_banks )%>% 
      select(Bank = name_1, `Similar Banks` = name_2, distance, Specialization = specialization_1)
    
  }
  
  if(limit_to_bank_class == FALSE & limit_to_specialization == FALSE){
    
    similar_banks <- bank_similarities %>% 
      filter(cert_1 == FDIC_certification_number) %>% 
      arrange(distance) %>% 
      slice(1:n_banks) %>% 
      select(Bank = name_1, `Similar Banks` = name_2, distance)
    
  }
  
  return(similar_banks)
}

```

Lets try getting similar banks to Bank of America without using any of the other groupings
```{r no groupings}
get_similar_banks(3510, n_banks = 5) %>% 
  kableExtra::kable() %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```

Now lets try the model fro CBI Bank & Trust and limit it to return only other banks with the same specialization
```{r limit specialization}
get_similar_banks(1552, n_banks = 5, limit_to_specialization = TRUE) %>% 
  kableExtra::kable() %>% 
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```
Notice the smaller banks show up much closer together than the large banks. This may be because most of the variables we used are right skewed. The large banks are so much larger than the small banks that their differences appear huge compared to the differences between the small banks. The log transform we tested may have helped to better separate the smaller banks. However, we would have to be okay with changing the nature of out data. 

Our MVP model seems to work decently well. It allows us to get similar banks to any input bank, subset output by specific charter classes and specializations, and gives us a starting point to iterate and improve upon. 

Finally a simple version of this model has been embedded in a web app [here](https://tpinckney27.shinyapps.io/banks-like-us-web-app). It does not have the categorical filtering options but will return them most similar banks for any given FDIC certificate number.


## Follow up Questions

### Advice to Coworkers
If a coworker was excited to use the model I would encourage them to do so with a few caveats and requests:

Caveats
1. This model is based on a subset of variables that someone with no subject matter expertise chose
2. It seems to group large banks fairly well, but may not perform as well with smaller banks

Requests:
1. Tell me what is missing. What other variables or sources of data should be included?
2. Play with the model and see if what it returns makes sense across a variety of banks.


### When Solution Should Not Be Used
There are at least two scenarios where extreme caution should be taken when using this model

1. When geographic location is important. This model does not take any geolocation features into account, so if you are a bank on the east coast and want to compare yourself only to other east coast banks this will not be helpful. 
2. If you want a list of similar banks for a small bank I would use caution. The model separates large banks from small banks well (that's a pretty easy task) but I am not sure how we'll it separates groups or types of small banks.


### Next Steps
* Talk to subject matter experts to understand what's missing
* Add in some variables related to geography or market overlap between banks
* Add in an auto select method for picking the number of similar banks
* Clean up the web app
* Find other sources of data to provide more variation
* Add other groupings to the function depending on potential use cases


### Homework Assignment Feedback
This was one of the more comprehensive technical challenges I've seen. I really enjoyed that I was given a real problem to solve, ability to pull in any data I could find, and use any tools I had at my disposal. In general I appreciate being given a time limit for a technical challenge. I feel like this levels the playing field and respects all applicants time equally. Unfortunately it also usually leads to a watering down of the challenge. 


Thank you for continuing to consider my application and giving me the opportunity to work on this challenge. I've really enjoyed continuing to learn about PrecisionLender as well as the opportunity to dive into Banking data. This challenge definitely opened my eyes to the treasure troves of Banking information just sitting out there publicly available. I would love to get some feedback on how I did in this challenge and look forward to speaking with you further.

Thank you
